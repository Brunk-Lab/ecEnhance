{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41839ac-1600-4bc8-8686-5ec32f289303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  \n",
    "import numpy as np  \n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd  \n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from bayes_opt import BayesianOptimization  # Bayesian-Optimization 1.2.0\n",
    "from bayes_opt.event import Events\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647bd492-9b55-41bd-9956-0d510dda8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_folder = r\"/work/users/b/e/behnamie/FACS-FISH_redistribution_NCIH2170/version_2_Acc_87/2_optimization/input_directory/ground_truth_RGB\"\n",
    "dapi_folder = r\"/work/users/b/e/behnamie/FACS-FISH_redistribution_NCIH2170/version_2_Acc_87/2_optimization/input_directory/ground_truth_DAPI\"\n",
    "ground_truth_csv = r\"/work/users/b/e/behnamie/FACS-FISH_redistribution_NCIH2170/version_2_Acc_87/2_optimization/input_directory/GT.csv\"\n",
    "mia_predictions_csv = r\"/work/users/b/e/behnamie/FACS-FISH_redistribution_NCIH2170/version_2_Acc_87/2_optimization/input_directory/output_mia.csv\"  # Adjust path as needed\n",
    "\n",
    "#Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_csv).sample(n=300, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1529436a-9905-4fe4-b555-def23a52b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_save_image(image, name, step, out_folder, unique_id=\"\"):\n",
    "    \"\"\"\n",
    "    Saves intermediate images for debugging purposes.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Image to save.\n",
    "    name (str): Descriptive name of the image.\n",
    "    step (int): Processing step number.\n",
    "    out_folder (str): Output directory.\n",
    "    unique_id (str): Unique identifier for the image.\n",
    "\n",
    "    Returns:\n",
    "    str: Filepath of the saved image.\n",
    "    \"\"\"\n",
    "    filename = f\"{unique_id}_{step:02d}_{name}.tif\" if unique_id else f\"{step:02d}_{name}.tif\"\n",
    "    filepath = os.path.join(out_folder, filename)\n",
    "    # cv2.imwrite(filepath, image)  # Uncomment to save debug images\n",
    "    print(f\"Saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def extract_unique_id(filename, suffixes):\n",
    "    \"\"\"\n",
    "    Extracts the base unique identifier from a filename by removing known suffixes.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Name of the file.\n",
    "    suffixes (list): List of possible suffixes to remove.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted unique identifier.\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    for suf in suffixes:\n",
    "        if base.endswith(suf):\n",
    "            return base[:-len(suf)]\n",
    "    return base\n",
    "\n",
    "def find_corresponding_dapi(unique_id, dapi_folder):\n",
    "    \"\"\"\n",
    "    Locates the corresponding DAPI image for a given RGB image based on unique ID.\n",
    "\n",
    "    Parameters:\n",
    "    unique_id (str): Unique identifier of the image.\n",
    "    dapi_folder (str): Directory containing DAPI images.\n",
    "\n",
    "    Returns:\n",
    "    str or None: Path to the DAPI image or None if not found.\n",
    "    \"\"\"\n",
    "    dapi_suffixes = [\"_DAPI\", \"_DAPI.tif\", \"_Merge.tif (RGB)\", \"_Merge.tif(RGB)\",\n",
    "                     \"_Merge.tif (RGB).tif\", \"_Merge.tif(RGB).tif\"]\n",
    "    for fname in os.listdir(dapi_folder):\n",
    "        if not fname.lower().endswith(('.tif', '.tiff', '.png')):\n",
    "            continue\n",
    "        uid = extract_unique_id(fname, dapi_suffixes)\n",
    "        if uid == unique_id:\n",
    "            return os.path.join(dapi_folder, fname)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd485bb3-d14c-49de-a356-4cdf27266aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_rgb_with_dapi(rgb_img, dapi_img):\n",
    "    \"\"\"\n",
    "    Masks the RGB image using the DAPI image to focus on the nucleus.\n",
    "\n",
    "    Parameters:\n",
    "    rgb_img (numpy.ndarray): RGB image.\n",
    "    dapi_img (numpy.ndarray): DAPI grayscale image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Masked RGB image.\n",
    "\n",
    "    Logic:\n",
    "    Pixels where the DAPI image is black (intensity = 0) are set to black in the RGB image,\n",
    "    isolating the region of interest (ROI) corresponding to the nucleus.\n",
    "    \"\"\"\n",
    "    if len(dapi_img.shape) != 2:\n",
    "        dapi_img = cv2.cvtColor(dapi_img, cv2.COLOR_BGR2GRAY)\n",
    "    mask = (dapi_img == 0)\n",
    "    rgb_masked = rgb_img.copy()\n",
    "    rgb_masked[mask] = 0\n",
    "    return rgb_masked\n",
    "\n",
    "def top_hat_enhancement(gray, kernel_size=20, chrom_kernel_size=200, dampening_factor=0.6):\n",
    "    \"\"\"\n",
    "    Enhances small bright features (ecDNAs) while suppressing large structures (chromosomes).\n",
    "\n",
    "    Parameters:\n",
    "    gray (numpy.ndarray): Grayscale image.\n",
    "    kernel_size (int): Size of the structuring element for top-hat transformation.\n",
    "    chrom_kernel_size (int): Size of the structuring element to estimate chromosomes.\n",
    "    dampening_factor (float): Factor (0 < factor < 1) to dampen chromosome regions.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Enhanced image with suppressed chromosomes.\n",
    "\n",
    "    Logic:\n",
    "\n",
    "    Top-Hat Transformation: Morphological opening removes small features, and subtraction from the original image highlights ecDNAs.\n",
    "    Chromosome Suppression: Morphological closing estimates large structures (chromosomes), \n",
    "    which are identified via Otsu thresholding and dampened to preserve nearby ecDNAs. \"\"\" \n",
    "\n",
    "    # Top-hat transformation\n",
    "    se = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(kernel_size), int(kernel_size)))\n",
    "    opened = cv2.morphologyEx(gray, cv2.MORPH_OPEN, se)\n",
    "    top_hat = cv2.subtract(gray, opened)\n",
    "    top_hat_norm = cv2.normalize(top_hat, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    \n",
    "    # Estimate chromosomes\n",
    "    chrom_se = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(chrom_kernel_size), int(chrom_kernel_size)))\n",
    "    chromosome_est = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, chrom_se)\n",
    "    _, chrom_mask = cv2.threshold(chromosome_est, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Dampen chromosome regions\n",
    "    top_hat_soft = top_hat_norm.astype(np.float32)\n",
    "    top_hat_soft[chrom_mask == 255] *= dampening_factor\n",
    "    return np.clip(top_hat_soft, 0, 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "def custom_clahe(gray, clip_limit, tile_grid_size):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization to enhance local contrast.\n",
    "\n",
    "    Parameters:\n",
    "    gray (numpy.ndarray): Grayscale image.\n",
    "    clip_limit (float): Threshold for contrast limiting.\n",
    "    tile_grid_size (int or tuple): Size of the grid for histogram equalization.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Enhanced image.\n",
    "\n",
    "    Logic:\n",
    "    CLAHE adjusts intensity histograms locally, improving visibility of ecDNAs against varying backgrounds.\n",
    "    \"\"\"\n",
    "    tile_size = (int(tile_grid_size), int(tile_grid_size))\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "def apply_sharpening_gray(gray, strength):\n",
    "    \"\"\"\n",
    "    Sharpens the image to enhance edges of ecDNAs.\n",
    "\n",
    "    Parameters:\n",
    "    gray (numpy.ndarray): Grayscale image.\n",
    "    strength (float): Sharpening intensity.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Sharpened image.\n",
    "\n",
    "    Logic:\n",
    "    A high-pass filter amplifies edge gradients, making ecDNA boundaries more distinct.\n",
    "    \"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32) * (strength / 5.0)\n",
    "    return cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "def apply_sigmoid(gray, cutoff, gain):\n",
    "    \"\"\"\n",
    "    Applies a sigmoid function to binarize bright spots.\n",
    "\n",
    "    Parameters:\n",
    "    gray (numpy.ndarray): Grayscale image.\n",
    "    cutoff (float): Intensity threshold for sigmoid.\n",
    "    gain (float): Steepness of the sigmoid curve.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Binarized image.\n",
    "    \n",
    "    Logic:\n",
    "    Maps intensities to a binary-like output, emphasizing ecDNAs as bright spots.\n",
    "    \"\"\"\n",
    "    norm = gray.astype(np.float32) / 255.0\n",
    "    c = cutoff / 255.0\n",
    "    out = 1.0 / (1.0 + np.exp(-gain * (norm - c)))\n",
    "    return (out * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def process_images(input_image, params, save_debug=False, output_folder=None, unique_id=\"\"):\n",
    "    \"\"\"\n",
    "    Processes the input image through the enhancement pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    input_image (numpy.ndarray): Input RGB or grayscale image.\n",
    "    [See individual function docstrings for other parameters]\n",
    "\n",
    "    Returns:\n",
    "    tuple: (simple_gray, enhanced) - Original grayscale and enhanced images.\n",
    "    \n",
    "    Logic:\n",
    "    Sequentially applies top-hat enhancement, CLAHE, sharpening, and sigmoid adjustment to isolate ecDNAs.\n",
    "    \"\"\" \n",
    "    if input_image.ndim == 3:\n",
    "        gray = cv2.cvtColor(cv2.convertScaleAbs(input_image), cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = cv2.convertScaleAbs(input_image)\n",
    "    \n",
    "    simple_gray = gray.copy()\n",
    "    th = top_hat_enhancement(gray, params['kernel_size'], params['chrom_kernel_size'], params['dampening_factor'])\n",
    "    sharp = apply_sharpening_gray(th, params['strength'])    \n",
    "    clahe_img = custom_clahe(sharp, params['clip_limit'], params['tile_grid_size'])\n",
    "    enhanced = apply_sigmoid(clahe_img, params['cutoff'], params['gain'])\n",
    "    \n",
    "    if save_debug and output_folder:\n",
    "        debug_save_image(simple_gray, \"simple_gray\", 1, output_folder, unique_id)\n",
    "        debug_save_image(enhanced, \"enhanced_gray\", 2, output_folder, unique_id)\n",
    "    \n",
    "    return simple_gray, enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56656168-97d0-4de6-a720-83e528030d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_close_objects(objects, merge_distance):\n",
    "    \"\"\"\n",
    "    Merges objects closer than a specified distance to avoid over-counting.\n",
    "\n",
    "    Parameters:\n",
    "    objects (list): List of detected objects with 'bbox', 'centroid', and 'area'.\n",
    "    merge_distance (float): Maximum distance to merge objects.\n",
    "    \n",
    "    Returns:\n",
    "    list: Merged objects.\n",
    "\n",
    "    Logic:\n",
    "    Combines nearby objects, likely fragments of the same ecDNA, based on centroid proximity.\n",
    "    \"\"\"\n",
    "\n",
    "    merged_objects = []\n",
    "    taken = [False] * len(objects)\n",
    "    for i in range(len(objects)):\n",
    "        if taken[i]:\n",
    "            continue\n",
    "        current = objects[i].copy()\n",
    "        for j in range(i + 1, len(objects)):\n",
    "            if taken[j]:\n",
    "                continue\n",
    "            if math.dist(current[\"centroid\"], objects[j][\"centroid\"]) < merge_distance:\n",
    "                x1, y1, w1, h1 = current[\"bbox\"]\n",
    "                x2, y2, w2, h2 = objects[j][\"bbox\"]\n",
    "                current[\"bbox\"] = (min(x1, x2), min(y1, y2), max(x1 + w1, x2 + w2) - min(x1, x2), max(y1 + h1, y2 + h2) - min(y1, y2))\n",
    "                cx1, cy1 = current[\"centroid\"]\n",
    "                cx2, cy2 = objects[j][\"centroid\"]\n",
    "                current[\"centroid\"] = ((cx1 + cx2) / 2.0, (cy1 + cy2) / 2.0)\n",
    "                current[\"area\"] += objects[j][\"area\"]\n",
    "                taken[j] = True\n",
    "        merged_objects.append(current)\n",
    "        taken[i] = True\n",
    "    return merged_objects\n",
    "\n",
    "def classify_as_white_or_ecDNA(roi, white_value_threshold=150, white_saturation_threshold=45):\n",
    "    \"\"\"\n",
    "    Classifies objects as 'chromosome' (white) or 'ecDNA' based on HSV color.\n",
    "\n",
    "    Parameters:\n",
    "    roi (numpy.ndarray): Region of interest from RGB image.\n",
    "    white_value_threshold (float): Minimum brightness (V) for white classification.\n",
    "    white_saturation_threshold (float): Maximum saturation (S) for white classification.\n",
    "\n",
    "    Returns:\n",
    "    str: 'chromosome' or 'ecDNA'.\n",
    "\n",
    "    Logic:\n",
    "    In HSV space, white objects (chromosomes) have high brightness (V) and low saturation (S),\n",
    "    while ecDNAs typically exhibit distinct colors.\n",
    "    \"\"\"\n",
    "\n",
    "    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    _, S_mean, V_mean, _ = cv2.mean(hsv_roi)\n",
    "    return \"chromosome\" if (V_mean > white_value_threshold and S_mean < white_saturation_threshold) else \"ecDNA\"\n",
    "\n",
    "def object_detection_and_overlay(enhanced_img, rgb_img_path, params, output_folder, unique_id):\n",
    "    \"\"\"\n",
    "    Detects objects in the enhanced image and classifies them using the RGB image.\n",
    "\n",
    "    Parameters:\n",
    "    enhanced_img (numpy.ndarray): Enhanced grayscale image.\n",
    "    rgb_img_path (str): Path to the corresponding RGB image.\n",
    "    [See individual function docstrings for other parameters]\n",
    "\n",
    "    Returns:\n",
    "    tuple: (total_count, merged_objects, counts) - Count of ecDNAs, detected objects, and classification counts.\n",
    "\n",
    "    Logic:\n",
    "    Uses connected components analysis to detect objects, merges close ones, and classifies them based on color.\n",
    "    \"\"\"\n",
    "    _, thresh = cv2.threshold(enhanced_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(cleaned, connectivity=8)\n",
    "    objects = [{\"bbox\": (stats[i, 0], stats[i, 1], stats[i, 2], stats[i, 3]), \"area\": stats[i, 4], \"centroid\": centroids[i]}\n",
    "               for i in range(1, num_labels) if params['min_area'] <= stats[i, 4] <= params['max_area']]\n",
    "    \n",
    "    merged_objects = merge_close_objects(objects, params['merge_distance'])\n",
    "    \n",
    "    if os.path.exists(rgb_img_path):\n",
    "        rgb_img = cv2.imread(rgb_img_path, cv2.IMREAD_COLOR)\n",
    "        counts = {\"chromosome\": 0, \"ecDNA\": 0}\n",
    "        for obj in merged_objects:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            roi = rgb_img[y:y+h, x:x+w]\n",
    "            label = classify_as_white_or_ecDNA(roi, params['white_value_threshold'], params['white_saturation_threshold'])\n",
    "            counts[label] += 1\n",
    "    else:\n",
    "        counts = {}\n",
    "    \n",
    "    return len(merged_objects), counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20e2f03-e47e-479a-b4df-e074d0d8edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_absolute_percentage_error(mape_list):\n",
    "    \"\"\"\n",
    "    Computes the median absolute percentage error (MdAPE).\n",
    "\n",
    "    Parameters:\n",
    "        mape_list (list): List of percentage errors.\n",
    "\n",
    "    Returns:\n",
    "        float: Median of the absolute percentage errors.\n",
    "\n",
    "    Logic:\n",
    "        MdAPE provides a robust central tendency measure, minimizing the impact of outliers in error distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.median(mape_list) if mape_list else 1e6\n",
    "\n",
    "def process_single_image(row, params):\n",
    "    \"\"\"\n",
    "    Processes a single image and computes its MAPE.\n",
    "\n",
    "    Parameters:\n",
    "        row (pandas.Series): Ground truth row with 'unique_id' and 'ground_truth_count'.\n",
    "        params (dict): Hyperparameters for the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        float: MAPE for the image.\n",
    "\n",
    "    Logic:\n",
    "        Applies the full pipeline to an image and calculates the percentage error against ground truth.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        true_count = row[\"ground_truth_count\"]\n",
    "        if true_count == 0:\n",
    "            return 1000  # High error for undefined MAPE\n",
    "\n",
    "        rgb_path = os.path.join(rgb_folder, f\"{unique_id}.tif\")\n",
    "        dapi_path = find_corresponding_dapi(unique_id, dapi_folder)\n",
    "\n",
    "        rgb_img = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
    "        if rgb_img is None:\n",
    "            return 1000\n",
    "\n",
    "        if dapi_path:\n",
    "            dapi_img = cv2.imread(dapi_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if dapi_img is not None:\n",
    "                rgb_img = mask_rgb_with_dapi(rgb_img, dapi_img)\n",
    "\n",
    "        _, enhanced = process_images(rgb_img, params)\n",
    "        total_count, _ = object_detection_and_overlay(enhanced, rgb_path, params, \"\", unique_id)\n",
    "        return 100.0 * abs(total_count - true_count) / true_count\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error (or print) and return a high error value\n",
    "        print(f\"Error processing image {row['unique_id']}: {e}\")\n",
    "        return 1000\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def objective_function(kernel_size, clip_limit, tile_grid_size, strength, cutoff, gain,\n",
    "                      merge_distance, min_area, max_area, white_value_threshold,\n",
    "                      white_saturation_threshold, chrom_kernel_size, dampening_factor):\n",
    "    \"\"\"\n",
    "    Objective function for Bayesian Optimization to minimize MdAPE.\n",
    "\n",
    "    Parameters:\n",
    "        [See individual function docstrings for parameter details]\n",
    "\n",
    "    Returns:\n",
    "        float: Negative MdAPE (for maximization in Bayesian Optimization).\n",
    "\n",
    "    Logic:\n",
    "        Evaluates the pipeline across all images in parallel, computing MdAPE to guide parameter optimization.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'kernel_size': kernel_size,\n",
    "        'strength': strength,\n",
    "        'clip_limit': clip_limit,\n",
    "        'tile_grid_size': tile_grid_size,\n",
    "        'cutoff': cutoff,\n",
    "        'gain': gain,\n",
    "        'chrom_kernel_size': chrom_kernel_size,\n",
    "        'dampening_factor': dampening_factor,\n",
    "        'merge_distance': merge_distance,\n",
    "        'min_area': min_area,\n",
    "        'max_area': max_area,\n",
    "        'white_value_threshold': white_value_threshold,\n",
    "        'white_saturation_threshold': white_saturation_threshold\n",
    "    }\n",
    "    \n",
    "    ground_truth_df = pd.read_csv(ground_truth_csv)\n",
    "    mape_values = []\n",
    "\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    max_workers = os.cpu_count() \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_image, row, params) for _, row in ground_truth_df.iterrows()]\n",
    "        for future in as_completed(futures):\n",
    "            mape_values.append(future.result())\n",
    "\n",
    "    \n",
    "    mdape = median_absolute_percentage_error(mape_values)\n",
    "    return -mdape  # Negative for maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19183b8-b54d-4a74-9f30-09935ea8cc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646eb63-5bd6-4327-bf95-74aeae4de072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | chrom_... | clip_l... |  cutoff   | dampen... |   gain    | kernel... | max_area  | merge_... | min_area  | strength  | tile_g... | white_... | white_... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pbounds = {\n",
    "        'kernel_size': (10, 50),\n",
    "        'strength': (2.0, 9.5),\n",
    "        'clip_limit': (0.1, 5.0),\n",
    "        'tile_grid_size': (10, 50),\n",
    "        'cutoff': (50, 150),\n",
    "        'gain': (1, 50),\n",
    "        'chrom_kernel_size': (100, 300),\n",
    "        'dampening_factor': (0.1, 0.9),\n",
    "        'merge_distance': (1, 20),\n",
    "        'min_area': (1, 20),\n",
    "        'max_area': (400, 1000),\n",
    "        'white_value_threshold': (100, 200),\n",
    "        'white_saturation_threshold': (20, 80)\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds=pbounds,\n",
    "        random_state=10,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Run optimization: 10 initial points for exploration, 50 iterations for refinement\n",
    "    optimizer.maximize(init_points=15, n_iter=85)\n",
    "    \n",
    "    print(\"Best Parameters:\", optimizer.max)\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = optimizer.max['params']\n",
    "    with open('best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecDNA Pipeline (Conda Python 3.9)",
   "language": "python",
   "name": "ecdna_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
